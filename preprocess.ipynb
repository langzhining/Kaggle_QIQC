{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lang\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\lang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "data_path = 'input/'\n",
    "word2vec_path = data_path+'embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 清理数据\n",
    "def clean_punct(s):\n",
    "    puncts = ',.\":)(-!?|;\\'$&/[]>%=#*+\\\\•~@£·_{}©^®`<→°€™›♥←×§″′Â█½à…“★”–●â►−¢²¬░¶↑±¿▾═¦║―¥▓—‹─▒：¼⊕▼▪†■’▀¨▄♫☆é¯♦¤▲è¸¾Ã⋅‘∞∙）↓、│（»，♪╩╚³・╦╣╔╗▬❤ïØ¹≤‡√'\n",
    "    for punct in puncts:\n",
    "        s = s.replace(punct, ' ')\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "def hash_number(s):\n",
    "    s = re.sub('[0-9]{5,}', '#####', s)\n",
    "    s = re.sub('[0-9]{4}', '####', s)\n",
    "    s = re.sub('[0-9]{3}', '###', s)\n",
    "    s = re.sub('[0-9]{2}', '##', s)\n",
    "    return s\n",
    "\n",
    "def clean_text(texts):\n",
    "    new_texts = []\n",
    "    for s in texts:\n",
    "        s = clean_punct(s)\n",
    "        s = hash_number(s)\n",
    "        s = s.lower()\n",
    "        new_texts.append(s)\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从已有词向量表中找到对应的单词，并生成子词典（tokenizer）\n",
    "def get_vocab_by_embed(full_tokenizer, embed_dict):\n",
    "    word_list = []\n",
    "    for word in full_tokenizer.word_counts.keys():\n",
    "        if word in embed_dict:\n",
    "            word_list.append(word)\n",
    "    words = ' '.join(word_list)\n",
    "    sub_vocab = Tokenizer(lower=False)\n",
    "    sub_vocab.fit_on_texts([words])\n",
    "    return sub_vocab\n",
    "\n",
    "# 生成与单词索引匹配的词向量\n",
    "def get_embedding_matrix(tokenizer, embed_dict):\n",
    "    vector_size = len(embed_dict['known'])\n",
    "    embedding_shape = (len(tokenizer.word_index)+1, vector_size)\n",
    "    embedding_matrix = np.zeros(embedding_shape)\n",
    "    indexes = []\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        embedding_matrix[index] = embed_dict[word]\n",
    "        indexes.append(index)\n",
    "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 将文本转化为tensor\n",
    "def texts_to_tensor(texts, tokenizer, maxlen=50):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    seqs_padded = pad_sequences(seqs, maxlen=maxlen, padding='post', truncating='pre', value=0)\n",
    "    seqs_padded = torch.tensor(seqs_padded, dtype=torch.int64)\n",
    "    mask = seqs_padded == 0\n",
    "    mask = mask\n",
    "    return seqs_padded, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 构建dataloader，用于torch训练和测试用\n",
    "def get_dataloader(x, mask, y=None,training=True, batch_size=32, \n",
    "                   weights=None, num_samples=None, drop_last=True):\n",
    "    if y is None:\n",
    "        data = TensorDataset(x, mask)\n",
    "    else:\n",
    "        data = TensorDataset(x, mask,y)\n",
    "    if training:\n",
    "        if weights is None:\n",
    "            sampler = RandomSampler(data)\n",
    "        else:\n",
    "            sampler = WeightedRandomSampler(weights=weights, num_samples=num_samples)\n",
    "    else:\n",
    "        sampler = SequentialSampler(data)\n",
    "    dataloader = DataLoader(data, sampler=sampler, shuffle=False, batch_size=batch_size, drop_last=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, heads=3, dropout=0.2):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        assert hidden_size % heads == 0\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.heads = heads\n",
    "        self.attn_size = int(hidden_size / heads)\n",
    "    \n",
    "    def transpose_for_scores(self, x, layer):\n",
    "        x = layer(x)\n",
    "        new_shape = x.size()[:-1] + (self.heads, self.attn_size)\n",
    "        x = x.view(*new_shape).permute(0, 2, 1, 3)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, hidden_stations, attention_mask):\n",
    "        # (batch_size, seq_len, hidden_size)\n",
    "        hidden_shape = hidden_stations.size()\n",
    "        # query: (batch_size, heads, seq_len, attn_size)\n",
    "        query = self.transpose_for_scores(hidden_stations, self.query)\n",
    "        key = self.transpose_for_scores(hidden_stations, self.key)\n",
    "        value = self.transpose_for_scores(hidden_stations, self.value)\n",
    "        \n",
    "        # (batch_size, heads, query_len, key_len)\n",
    "        attention_weight = torch.matmul(query, key.transpose(-1,-2)) / math.sqrt(self.attn_size)\n",
    "        attention_weight = attention_weight.masked_fill_(attention_mask, -1e9)\n",
    "        attention_weight = nn.Softmax(dim=-1)(attention_weight)\n",
    "        \n",
    "        attention_weith = self.dropout(attention_weight)\n",
    "        \n",
    "        # (batch_size, heads, query_len, attn_size)\n",
    "        context = torch.matmul(attention_weight, value)\n",
    "        context = context.permute(0,2,1,3).contiguous()\n",
    "        context = context.view(*hidden_shape)\n",
    "        return context\n",
    "\n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filters, seq_len, dropout=0):\n",
    "        super(CNNLayer, self).__init__()\n",
    "        self.layer_1 = nn.Conv2d(1, filters, kernel_size=(1, hidden_size))\n",
    "        self.layer_2 = nn.Conv2d(filters, filters, kernel_size=(seq_len, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(seq_len, 1))\n",
    "        self.dense = nn.Linear(2*filters, 1)\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        self.filters = filters\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # (batch_size, 1, seq_len, hidden_size)\n",
    "        x = x.unsqueeze(1)\n",
    "        # (batch_size, filters, seq_len, 1)\n",
    "        conv1 = F.relu(self.layer_1(x))\n",
    "        # (batch_size, filters, 1, 1)\n",
    "        out1 = F.relu(self.layer_2(conv1)).squeeze(2).squeeze(2)\n",
    "        out2 = self.maxpool(conv1).squeeze(2).squeeze(2)\n",
    "        out = torch.cat([out1, out2], dim=1)\n",
    "        out = self.dense(out)\n",
    "        return out\n",
    "    \n",
    "class MixModel(nn.Module):\n",
    "    def __init__(self, attn_layer, cnn_layer, pre_train_embedding, freeze=False):\n",
    "        super(MixModel, self).__init__()\n",
    "#         self.attn_layer = attn_layer\n",
    "#         self.cnn_layer = cnn_layer\n",
    "        self.embed_layer = nn.Embedding.from_pretrained(pre_train_embedding, freeze=freeze)\n",
    "        self.linear == nn.Linear(300, 1)\n",
    "#         self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        embed = self.embed_layer(x)\n",
    "        embed = embed.mean(dim=1)\n",
    "        out = self.linear(embed)\n",
    "#         embed = self.embed_layer(x)\n",
    "#         # (batch, seq_len, hidden_size)\n",
    "#         attention_mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "#         attn = self.attn_layer(embed, attention_mask)\n",
    "#         cnn_mask = mask.unsqueeze(2)\n",
    "#         cnn_input = attn.masked_fill_(cnn_mask, 0)\n",
    "#         out = self.cnn_layer(cnn_input)\n",
    "        return out\n",
    "    \n",
    "    def predict(self, dataloader):\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                X_batch, mask= batch\n",
    "                preds.append(self.forward(X_batch, mask).data.cpu())\n",
    "        return torch.cat(preds)\n",
    "\n",
    "    def predict_proba(self, dataloader):\n",
    "        return torch.sigmoid(self.predict(dataloader)).data.numpy()\n",
    "\n",
    "def model_build(embedding_matrix, heads=2, max_seq_len=50, cnn_filter=128):\n",
    "    embedding_size = len(embedding_matrix[0])\n",
    "    attn_layer = SelfAttention(hidden_size=embedding_size, heads=heads, dropout=0.2)\n",
    "    cnn_layer = CNNLayer(hidden_size=embedding_size, filters=128, seq_len=max_seq_len)\n",
    "    model = MixModel(attn_layer, cnn_layer, pre_train_embedding=embedding_matrix,freeze=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, optimizer, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss(), verbose_step=10000):\n",
    "    t1 = time.time()\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        x_batch, m_batch, y_batch = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(x_batch, m_batch)\n",
    "        loss = criterion(outputs[:, 0], y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_batch_end(model)\n",
    "        if (step + 1) % verbose_step == 0:\n",
    "            loss_now = tr_loss / (step + 1)\n",
    "            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n",
    "    if callbacks is not None:\n",
    "        for func in callbacks:\n",
    "            func.on_epoch_end(model)\n",
    "    return tr_loss / (step + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_path+'train.csv')\n",
    "train_texts = list(train_df.question_text.values)\n",
    "train_texts = clean_text(train_texts)\n",
    "\n",
    "\n",
    "word2vec_dict = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "full_tokenizer = Tokenizer(lower=False)\n",
    "full_tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "word2vec_tokenizer = get_vocab_by_embed(full_tokenizer, word2vec_dict)\n",
    "\n",
    "word2vec_matrix = get_embedding_matrix(word2vec_tokenizer, word2vec_dict)\n",
    "\n",
    "train_x, train_mask = texts_to_tensor(train_texts, word2vec_tokenizer)\n",
    "train_y = torch.tensor(train_df.target.values, dtype=torch.float32)\n",
    "\n",
    "train_dataloader = get_dataloader(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_x = train_x[1000000:]\n",
    "eval_y = train_y[1000000:]\n",
    "eval_m = train_mask[1000000:]\n",
    "x = train_x[:1000000]\n",
    "y = train_y[:1000000]\n",
    "m = train_mask[:1000000]\n",
    "train_loader = get_dataloader(x, m,y, batch_size=100, training=True)\n",
    "eval_loder = get_dataloader(eval_x, eval_m, batch_size=100, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MixModel' object has no attribute 'linear'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-e5a607ba9cd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# device = torch.device('cpu')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-b5c8281a6fd4>\u001b[0m in \u001b[0;36mmodel_build\u001b[1;34m(embedding_matrix, heads, max_seq_len, cnn_filter)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mattn_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelfAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mcnn_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNNLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_seq_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMixModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_train_embedding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-b5c8281a6fd4>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, attn_layer, cnn_layer, pre_train_embedding, freeze)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;31m#         self.cnn_layer = cnn_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_train_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;31m#         self.init_weights()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 518\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MixModel' object has no attribute 'linear'"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else  torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "model = model_build(word2vec_matrix)\n",
    "optimizer = Adam(model.parameters())\n",
    "model.cuda()\n",
    "for _ in range(1):\n",
    "    model.train()\n",
    "    \n",
    "    loss = run_epoch(model, train_loader, optimizer)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixModel(\n",
       "  (attn_layer): SelfAttention(\n",
       "    (query): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (key): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (value): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (dropout): Dropout(p=0.2)\n",
       "  )\n",
       "  (cnn_layer): CNNLayer(\n",
       "    (layer_1): Conv2d(1, 128, kernel_size=(1, 10), stride=(1, 1))\n",
       "    (layer_2): Conv2d(128, 128, kernel_size=(50, 1), stride=(1, 1))\n",
       "    (maxpool): MaxPool2d(kernel_size=(50, 1), stride=(50, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (dense): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (embed_layer): Embedding(76655, 10)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class test_model(nn.Module):\n",
    "    def __init__(self, pre_train_embedding, freeze=False):\n",
    "        super(test_model, self).__init__()\n",
    "        self.embed_layer = nn.Embedding.from_pretrained(pre_train_embedding, freeze=freeze)\n",
    "        self.linear = nn.Linear(300, 1)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        embed = self.embed_layer(x)\n",
    "        out = self.linear(embed)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_loader):\n",
    "#     batch = tuple(t.to(device) for t in batch)\n",
    "    x_batch, m_batch, y_batch = batch\n",
    "#     model.zero_grad()\n",
    "#     outputs = model(x_batch, m_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,4,300).dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
