{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "from bulid_model import *\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 6]), tensor([6, 6, 5]), torch.Size([3, 10]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,10)\n",
    "a[:-1,-4:] = 0\n",
    "a[-1,-4:] = 0\n",
    "# lengths = torch.tensor([5,6,6])\n",
    "lengths = [6,6,5]\n",
    "b = torch.nn.utils.rnn.pack_padded_sequence(a, lengths, batch_first=True)\n",
    "c,new_lengths = torch.nn.utils.rnn.pad_packed_sequence(b,batch_first=True)\n",
    "c.shape, new_lengths, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_mask,train_y, word2vec_embedding = preprocess.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, dataloader, optimizer, callbacks=None,\n",
    "              criterion=nn.BCEWithLogitsLoss(), verbose_step=100):\n",
    "    t1 = time.time()\n",
    "    tr_loss = 0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        x_batch, m_batch, y_batch = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(x_batch, m_batch)\n",
    "        loss = criterion(outputs[:, 0], y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        if callbacks is not None:\n",
    "            for func in callbacks:\n",
    "                func.on_batch_end(model)\n",
    "        if (step + 1) % verbose_step == 0:\n",
    "            loss_now = tr_loss / (step + 1)\n",
    "            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n",
    "    if callbacks is not None:\n",
    "        for func in callbacks:\n",
    "            func.on_epoch_end(model)\n",
    "    return tr_loss / (step + 1)\n",
    "\n",
    "\n",
    "def eval_data(model, dataloader, y_eval, threshold=0.3):\n",
    "    y_prob = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            x_batch, m_batch = batch\n",
    "            outputs = model(x_batch, m_batch)\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            y_prob.append(outputs.data.cpu())\n",
    "    y_prob = torch.cat(y_prob).data.numpy()\n",
    "    y_pred = (y_prob>threshold).astype(int)\n",
    "    f1 = f1_score(y_eval, y_pred)\n",
    "    print('eval f1 socre:', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, pretrained_embedding, proj_dim=128, rnn_dim=128, n_layers=1, bidirectional=False,\n",
    "                 padding_idx=0, fix_embedding=False,\n",
    "                 n_out=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embed_dim = len(pretrained_embedding[0])\n",
    "        self.n_layers = n_layers\n",
    "        self.dense_dim = rnn_dim * 2 if bidirectional else rnn_dim\n",
    "        self.n_out = n_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.fix_embedding = fix_embedding\n",
    "        self.padding_idx = padding_idx\n",
    "#         if pretrained_embedding is not None:\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embedding, freeze=fix_embedding)\n",
    "        self.embed.padding_idx = self.padding_idx\n",
    "#         else:\n",
    "#             self.embed = nn.Embedding(self.n_vocab, self.embed_dim, padding_idx=self.padding_idx)\n",
    "        self.proj = nn.Linear(self.embed_dim, proj_dim)\n",
    "        self.proj_act = nn.ReLU()\n",
    "        self.gru = nn.GRU(proj_dim, rnn_dim, self.n_layers,\n",
    "                          batch_first=True, bidirectional=bidirectional)\n",
    "        self.pooling = GlobalMaxPooling1D()\n",
    "        in_dim = 2 * rnn_dim if self.bidirectional else rnn_dim\n",
    "        self.dense = nn.Linear(in_dim, self.dense_dim)\n",
    "        self.dense_act = nn.ReLU()\n",
    "        self.out_linear = nn.Linear(self.dense_dim, n_out)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name.find('embed') > -1:\n",
    "                continue\n",
    "            elif name.find('weight') > -1 and len(param.size()) > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, inputs, mask=None):\n",
    "        # inputs: (bs, max_len)\n",
    "        x = self.embed(inputs)\n",
    "        x = self.proj_act(self.proj(x))\n",
    "        x, hidden = self.gru(x)\n",
    "        x = self.pooling(x)\n",
    "        x = self.dense_act(self.dense(x))\n",
    "        x = self.out_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, heads=3, seq_len=50, dropout=0.2):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        assert hidden_size % heads == 0\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(seq_len, 1))\n",
    "        self.out_layer = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.heads = heads\n",
    "        self.attn_size = int(hidden_size / heads)\n",
    "    \n",
    "    def transpose_for_scores(self, x, layer):\n",
    "        x = layer(x)\n",
    "        new_shape = x.size()[:-1] + (self.heads, self.attn_size)\n",
    "        x = x.view(*new_shape).permute(0, 2, 1, 3)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, hidden_stations, attention_mask):\n",
    "        # (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "        hidden_shape = hidden_stations.size()\n",
    "        # query: (batch_size, heads, seq_len, attn_size)\n",
    "        query = self.transpose_for_scores(hidden_stations, self.query)\n",
    "        key = self.transpose_for_scores(hidden_stations, self.key)\n",
    "        value = self.transpose_for_scores(hidden_stations, self.value)\n",
    "        \n",
    "        # (batch_size, heads, query_len, key_len)\n",
    "        attention_weight = torch.matmul(query, key.transpose(-1,-2)) / math.sqrt(self.attn_size)\n",
    "        attention_weight = attention_weight.masked_fill_(attention_mask, -1e9)\n",
    "        attention_weight = nn.Softmax(dim=-1)(attention_weight)\n",
    "        \n",
    "        attention_weith = self.dropout(attention_weight)\n",
    "        \n",
    "        # (batch_size, heads, query_len, attn_size)\n",
    "        context = torch.matmul(attention_weight, value)\n",
    "        context = context.permute(0,2,1,3).contiguous()\n",
    "        context = context.view(*hidden_shape)\n",
    "        \n",
    "        out = torch.relu(context)\n",
    "             \n",
    "#         context = context.unsqueeze(1)\n",
    "#         context = self.maxpool(context).squeeze()\n",
    "#         out = self.out_layer(context)\n",
    "        return  out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, hidden_size, filters, seq_len, dropout=0.2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer_1 = nn.Conv2d(1, filters, kernel_size=(1, hidden_size))\n",
    "#         self.layer_2 = nn.Conv2d(filters, filters, kernel_size=(seq_len, 1))\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(seq_len, 1))\n",
    "        self.dense = nn.Linear(filters, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.filters = filters\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # (batch_size, 1, seq_len, hidden_size)\n",
    "        x = x.unsqueeze(1)\n",
    "        cnn_mask = mask.unsqueeze(1).unsqueeze(3)\n",
    "        x = x.masked_fill_(cnn_mask, 0)\n",
    "        # (batch_size, filters, seq_len, 1)\n",
    "        conv1 = F.relu(self.layer_1(x))\n",
    "        # (batch_size, filters, 1, 1)\n",
    "#         out1 = F.relu(self.layer_2(conv1)).squeeze(2).squeeze(2)\n",
    "        out = self.maxpool(conv1).squeeze()\n",
    "#         out = torch.cat([out1, out2], dim=1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.dense(out)\n",
    "        return out\n",
    "    \n",
    "class AttnCNNModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, layers, freeze=False):\n",
    "        super(AttnCNNModel, self).__init__()\n",
    "        self.embed_layer = nn.Embedding.from_pretrained(embedding_matrix, freeze=freeze)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        out = self.embed_layer(x)\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        return out\n",
    "    \n",
    "def model_build(embedding_matrix, heads=6, max_seq_len=50, cnn_filter=128):\n",
    "    embedding_size = len(embedding_matrix[0])\n",
    "    attn_layer = SelfAttention(hidden_size=embedding_size, heads=heads, seq_len=max_seq_len,dropout=0.2)\n",
    "    cnn_layer = CNN(embedding_size, cnn_filter, max_seq_len, dropout=0.2)\n",
    "    model = AttnCNNModel(embedding_matrix, [attn_layer,cnn_layer])\n",
    "#     cnn_layer = CNNLayer(hidden_size=embedding_size, filters=128, seq_len=max_seq_len)\n",
    "#     model = MixModel(attn_layer, cnn_layer, pre_train_embedding=embedding_matrix,freeze=False)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_x = train_x.cuda()\n",
    "# train_y = train_y.cuda()\n",
    "# train_mask = train_mask.cuda()\n",
    "eval_x = train_x[1000000:]\n",
    "eval_y = train_y[1000000:]\n",
    "eval_m = train_mask[1000000:]\n",
    "x = train_x[:1000000]\n",
    "y = train_y[:1000000]\n",
    "m = train_mask[:1000000]\n",
    "train_loader = preprocess.get_dataloader(x, m,y, batch_size=500, training=True)\n",
    "eval_loder = preprocess.get_dataloader(eval_x, eval_m,batch_size=500, training=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([306122, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:100 loss:0.2252112 time:6.2s\n",
      "step:200 loss:0.1779917 time:12.4s\n",
      "step:300 loss:0.1611622 time:18.5s\n",
      "step:400 loss:0.1510682 time:24.6s\n",
      "step:500 loss:0.1444479 time:30.8s\n",
      "step:600 loss:0.1398672 time:36.9s\n",
      "step:700 loss:0.1366521 time:43.2s\n",
      "step:800 loss:0.1341730 time:49.4s\n",
      "step:900 loss:0.1319987 time:55.5s\n",
      "step:1000 loss:0.1301171 time:61.7s\n",
      "step:1100 loss:0.1281581 time:67.9s\n",
      "step:1200 loss:0.1267251 time:74.1s\n",
      "step:1300 loss:0.1254922 time:80.3s\n",
      "step:1400 loss:0.1244053 time:86.5s\n",
      "step:1500 loss:0.1232737 time:92.7s\n",
      "step:1600 loss:0.1221937 time:98.8s\n",
      "step:1700 loss:0.1215380 time:105.0s\n",
      "step:1800 loss:0.1206982 time:111.3s\n",
      "step:1900 loss:0.1200224 time:117.5s\n",
      "step:2000 loss:0.1192632 time:123.6s\n",
      "eval f1 socre: 0.6502993409468231\n",
      "step:100 loss:0.0945010 time:6.4s\n",
      "step:200 loss:0.0932860 time:12.5s\n",
      "step:300 loss:0.0932547 time:18.7s\n",
      "step:400 loss:0.0930474 time:24.9s\n",
      "step:500 loss:0.0929594 time:31.1s\n",
      "step:600 loss:0.0931829 time:37.3s\n",
      "step:700 loss:0.0935115 time:43.6s\n",
      "step:800 loss:0.0938734 time:49.9s\n",
      "step:900 loss:0.0937345 time:56.1s\n",
      "step:1000 loss:0.0938132 time:62.3s\n",
      "step:1100 loss:0.0936623 time:68.5s\n",
      "step:1200 loss:0.0938573 time:74.7s\n",
      "step:1300 loss:0.0940219 time:80.9s\n",
      "step:1400 loss:0.0940902 time:87.1s\n",
      "step:1500 loss:0.0939812 time:93.3s\n",
      "step:1600 loss:0.0939586 time:99.5s\n",
      "step:1700 loss:0.0943328 time:105.7s\n",
      "step:1800 loss:0.0942916 time:111.9s\n",
      "step:1900 loss:0.0944282 time:118.1s\n",
      "step:2000 loss:0.0945432 time:124.2s\n",
      "eval f1 socre: 0.6540990793511617\n",
      "step:100 loss:0.0747596 time:6.4s\n",
      "step:200 loss:0.0747193 time:12.6s\n",
      "step:300 loss:0.0746007 time:18.8s\n",
      "step:400 loss:0.0755371 time:25.1s\n",
      "step:500 loss:0.0757230 time:31.4s\n",
      "step:600 loss:0.0760799 time:37.7s\n",
      "step:700 loss:0.0760041 time:44.0s\n",
      "step:800 loss:0.0763087 time:50.1s\n",
      "step:900 loss:0.0762636 time:56.3s\n",
      "step:1000 loss:0.0769249 time:62.6s\n",
      "step:1100 loss:0.0770448 time:68.9s\n",
      "step:1200 loss:0.0775470 time:75.2s\n",
      "step:1300 loss:0.0779415 time:81.5s\n",
      "step:1400 loss:0.0781315 time:87.7s\n",
      "step:1500 loss:0.0783407 time:93.8s\n",
      "step:1600 loss:0.0784296 time:100.0s\n",
      "step:1700 loss:0.0785294 time:106.3s\n",
      "step:1800 loss:0.0788074 time:112.5s\n",
      "step:1900 loss:0.0790164 time:118.8s\n",
      "step:2000 loss:0.0790239 time:125.2s\n",
      "eval f1 socre: 0.6458729926294724\n",
      "step:100 loss:0.0582443 time:6.3s\n",
      "step:200 loss:0.0586013 time:12.5s\n",
      "step:300 loss:0.0594934 time:18.7s\n",
      "step:400 loss:0.0592811 time:25.0s\n",
      "step:500 loss:0.0594296 time:31.3s\n",
      "step:600 loss:0.0598555 time:37.5s\n",
      "step:700 loss:0.0602765 time:43.8s\n",
      "step:800 loss:0.0608373 time:50.1s\n",
      "step:900 loss:0.0610979 time:56.2s\n",
      "step:1000 loss:0.0613256 time:62.4s\n",
      "step:1100 loss:0.0615092 time:68.7s\n",
      "step:1200 loss:0.0619033 time:74.9s\n",
      "step:1300 loss:0.0621718 time:81.0s\n",
      "step:1400 loss:0.0623863 time:87.3s\n",
      "step:1500 loss:0.0625863 time:93.6s\n",
      "step:1600 loss:0.0628044 time:99.9s\n",
      "step:1700 loss:0.0631352 time:106.2s\n",
      "step:1800 loss:0.0632894 time:112.4s\n",
      "step:1900 loss:0.0634910 time:118.6s\n",
      "step:2000 loss:0.0635843 time:124.9s\n",
      "eval f1 socre: 0.6319526035547335\n",
      "step:100 loss:0.0445090 time:6.3s\n",
      "step:200 loss:0.0449262 time:12.5s\n",
      "step:300 loss:0.0454789 time:18.8s\n",
      "step:400 loss:0.0451445 time:25.0s\n",
      "step:500 loss:0.0456186 time:31.1s\n",
      "step:600 loss:0.0461324 time:37.3s\n",
      "step:700 loss:0.0464757 time:43.5s\n",
      "step:800 loss:0.0466256 time:49.7s\n",
      "step:900 loss:0.0468280 time:56.0s\n",
      "step:1000 loss:0.0471662 time:62.1s\n",
      "step:1100 loss:0.0475063 time:68.2s\n",
      "step:1200 loss:0.0480593 time:74.4s\n",
      "step:1300 loss:0.0482741 time:80.6s\n",
      "step:1400 loss:0.0484705 time:86.8s\n",
      "step:1500 loss:0.0487149 time:92.9s\n",
      "step:1600 loss:0.0489554 time:99.1s\n",
      "step:1700 loss:0.0491294 time:105.2s\n",
      "step:1800 loss:0.0494323 time:111.4s\n",
      "step:1900 loss:0.0497073 time:117.6s\n",
      "step:2000 loss:0.0499135 time:123.8s\n",
      "eval f1 socre: 0.6222154564367991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-103b4b5af8b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0meval_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_loder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-246f5bfbe369>\u001b[0m in \u001b[0;36mrun_epoch\u001b[1;34m(model, dataloader, optimizer, callbacks, criterion, verbose_step)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    571\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                                                   reduction=self.reduction)\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   1660\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1661\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'elementwise_mean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1662\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1663\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1664\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else  torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "# model = GRUModel(word2vec_embedding)\n",
    "model = model_build(word2vec_embedding, )\n",
    "optimizer = Adam(model.parameters())\n",
    "model.cuda()\n",
    "for _ in range(2):\n",
    "    model.train()\n",
    "    loss = run_epoch(model, train_loader, optimizer)\n",
    "    model.eval()\n",
    "    eval_data(model, eval_loder, y_eval=eval_y, threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7059791962882586"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(eval_y.data.numpy(), df.prediction.values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
